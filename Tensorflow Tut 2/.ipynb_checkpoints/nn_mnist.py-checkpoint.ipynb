{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Aishik Chakraborty\n",
    "Using checkpointing in Tensorflow\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "tf.set_random_seed(123)  # reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n",
      "(5500, 784)\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:global_step/sec: 0\n",
      "Epoch : 0 Training Loss: 0.271696949472\n",
      "Validation Accuracy: 0.961091\n",
      "Epoch : 1 Training Loss: 0.123334100549\n",
      "Validation Accuracy: 0.972909\n",
      "Epoch : 2 Training Loss: 0.084519075892\n",
      "Validation Accuracy: 0.976182\n",
      "Epoch : 3 Training Loss: 0.0630652308558\n",
      "Validation Accuracy: 0.976363\n",
      "Epoch : 4 Training Loss: 0.052724371398\n",
      "Validation Accuracy: 0.976\n",
      "Epoch : 5 Training Loss: 0.0418031172408\n",
      "Validation Accuracy: 0.981091\n",
      "Epoch : 6 Training Loss: 0.0377528071046\n",
      "Validation Accuracy: 0.980364\n",
      "Epoch : 7 Training Loss: 0.0312446936433\n",
      "Validation Accuracy: 0.981091\n",
      "Epoch : 8 Training Loss: 0.0264482819275\n",
      "Validation Accuracy: 0.977636\n",
      "Epoch : 9 Training Loss: 0.0289179145199\n",
      "Validation Accuracy: 0.978182\n",
      "Epoch : 10 Training Loss: 0.0272422314422\n",
      "Validation Accuracy: 0.981818\n",
      "Epoch : 11 Training Loss: 0.0221351460729\n",
      "Validation Accuracy: 0.979636\n",
      "Epoch : 12 Training Loss: 0.023683575169\n",
      "Validation Accuracy: 0.981454\n",
      "Epoch : 13 Training Loss: 0.018069447294\n",
      "Validation Accuracy: 0.984545\n",
      "Epoch : 14 Training Loss: 0.0174015106957\n",
      "Validation Accuracy: 0.979636\n",
      "Epoch : 15 Training Loss: 0.0178375439265\n",
      "Validation Accuracy: 0.982545\n",
      "Epoch : 16 Training Loss: 0.0162877612425\n",
      "Validation Accuracy: 0.981091\n",
      "Epoch : 17 Training Loss: 0.0154099526828\n",
      "INFO:tensorflow:global_step/sec: 74.2496\n",
      "Validation Accuracy: 0.983818\n",
      "Epoch : 18 Training Loss: 0.0143696819725\n",
      "Validation Accuracy: 0.982545\n",
      "Epoch : 19 Training Loss: 0.0143236431477\n",
      "Validation Accuracy: 0.982182\n",
      "Epoch : 20 Training Loss: 0.0137924800099\n",
      "Validation Accuracy: 0.980182\n",
      "Epoch : 21 Training Loss: 0.0123358023358\n",
      "Validation Accuracy: 0.980363\n",
      "Epoch : 22 Training Loss: 0.0133525870526\n",
      "Validation Accuracy: 0.981273\n",
      "Epoch : 23 Training Loss: 0.0128549683349\n",
      "Validation Accuracy: 0.983091\n",
      "Epoch : 24 Training Loss: 0.0102277101356\n",
      "Validation Accuracy: 0.980545\n",
      "Early stopping ...\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "train_data = mnist.train.images  # Returns np.array\n",
    "train_labels = np.eye(10)[np.asarray(mnist.train.labels, dtype=np.int32)]\n",
    "test_data = mnist.test.images  # Returns np.array\n",
    "test_labels = np.eye(10)[np.asarray(mnist.test.labels, dtype=np.int32)]\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.1, random_state=123)\n",
    "\n",
    "print val_data.shape\n",
    "learning_rate = 0.01\n",
    "training_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "class NN(object):\n",
    "    def __init__(self):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            # input place holders\n",
    "            self.X = tf.placeholder(tf.float32, [None, 784])\n",
    "            self.Y = tf.placeholder(tf.float32, [None, 10])\n",
    "            self.mode = tf.placeholder(tf.bool)\n",
    "            self.keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "            self.h1 = tf.layers.dense(inputs=self.X, units=500)\n",
    "            self.h1 = tf.layers.batch_normalization(self.h1, training=self.mode)\n",
    "            self.h1 = tf.nn.relu(self.h1)\n",
    "            self.h1 = tf.nn.dropout(self.h1, self.keep_prob)\n",
    "            self.h2 = tf.layers.dense(inputs=self.h1, units=500)\n",
    "            self.h2 = tf.layers.batch_normalization(self.h2, training=self.mode)\n",
    "            self.h2 = tf.nn.relu(self.h2)\n",
    "            self.h2 = tf.nn.dropout(self.h2, self.keep_prob)\n",
    "\n",
    "            self.logits = tf.layers.dense(inputs=self.h2, units=10)\n",
    "            self.logits = tf.layers.batch_normalization(self.logits, training=self.mode)\n",
    "            self.pred = tf.nn.softmax(self.logits)\n",
    "            # Test model and check accuracy\n",
    "            self.correct_prediction = tf.equal(tf.argmax(self.pred, 1), tf.argmax(self.Y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "\n",
    "            tf.summary.scalar('accuracy', self.accuracy)\n",
    "            # define cost/loss & optimizer\n",
    "            self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.Y))\n",
    "            self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            tf.summary.scalar('mean_loss', self.cost)\n",
    "            self.merged = tf.summary.merge_all()\n",
    "\n",
    "            # When using the batchnormalization layers,\n",
    "            # it is necessary to manually add the update operations\n",
    "            # because the moving averages are not included in the graph            \n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(update_ops):                     \n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.cost, global_step=self.global_step)\n",
    "\n",
    "        \n",
    "nn = NN()\n",
    "# Best validation accuracy seen so far.\n",
    "best_validation_accuracy = 0.0\n",
    "\n",
    "# Iteration-number for last improvement to validation accuracy.\n",
    "last_improvement = 0\n",
    "\n",
    "# Stop optimization if no improvement found in this many iterations.\n",
    "patience = 10\n",
    "\n",
    "# Start session\n",
    "sv = tf.train.Supervisor(graph=nn.graph,\n",
    "                         logdir='logs/',\n",
    "                         summary_op=None,\n",
    "                         save_model_secs=0)\n",
    "\n",
    "with sv.managed_session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(len(train_data) / batch_size)\n",
    "        if sv.should_stop(): break\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = train_data[(i)*batch_size:(i+1)*batch_size], train_labels[(i)*batch_size:(i+1)*batch_size]\n",
    "            feed_dict = {nn.X: batch_xs, nn.Y: batch_ys, nn.mode:True, nn.keep_prob:0.8}\n",
    "            c, _ = sess.run([nn.cost, nn.optimizer], feed_dict=feed_dict)\n",
    "            avg_cost += c / total_batch\n",
    "            if i%50:\n",
    "                \n",
    "                sv.summary_computed(sess, sess.run(nn.merged, feed_dict))\n",
    "                gs = sess.run(nn.global_step, feed_dict)\n",
    "        \n",
    "        print 'Epoch : ' + str(epoch) + ' Training Loss: ' + str(avg_cost)\n",
    "        acc = sess.run(nn.accuracy, feed_dict={\n",
    "                        nn.X: val_data, nn.Y: val_labels, nn.mode:False, nn.keep_prob:1.0})\n",
    "        print 'Validation Accuracy: ' + str(acc)\n",
    "        if acc > best_validation_accuracy:\n",
    "            last_improvement = epoch\n",
    "            best_validation_accuracy = acc\n",
    "            sv.saver.save(sess, 'logs' + '/model_gs', global_step=gs)\n",
    "        if epoch - last_improvement > patience:\n",
    "            print(\"Early stopping ...\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph loaded\n",
      "INFO:tensorflow:Starting standard services.\n",
      "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:Restoring parameters from logs/model_gs-6930\n",
      "Restored!\n",
      "('Accuracy:', 0.98120016)\n"
     ]
    }
   ],
   "source": [
    "# Load graph\n",
    "nn = NN()\n",
    "print(\"Graph loaded\")\n",
    "with nn.graph.as_default():\n",
    "    sv = tf.train.Supervisor()\n",
    "    with sv.managed_session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "        ## Restore parameters\n",
    "        sv.saver.restore(sess, tf.train.latest_checkpoint('logs/'))\n",
    "        print(\"Restored!\")\n",
    "        acc = sess.run(nn.accuracy, feed_dict={\n",
    "              nn.X: test_data, nn.Y: test_labels, nn.mode:False, nn.keep_prob:1.0})\n",
    "        print('Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
